# -*- coding: utf-8 -*-
"""Plasmid_Chromosome_Model 0.5.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Eljp3HfrqUybjzr9-qRoyw8ApTH-ESLg
"""

import os
import sys
import re
from tqdm import tqdm
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from numpy.random import randint
import torch
import torch.utils.data
from torch.utils.data import random_split
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data.dataloader import DataLoader
import matplotlib
import matplotlib.pyplot as plt
from HDF5dataset import HDF5Dataset
from WeightsCreator import make_weights_for_balanced_classes

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')
    
def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)

def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def plot_accuracies(history):
    accuracies = [x['val_acc'] for x in history]
    plt.plot(accuracies, '-x')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.title('Accuracy vs. No. of epochs');

def plot_losses(history):
    train_losses = [x.get('train_loss') for x in history]
    val_losses = [x['val_loss'] for x in history]
    plt.plot(train_losses, '-bx')
    plt.plot(val_losses, '-rx')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['Training', 'Validation'])
    plt.title('Loss vs. No. of epochs');

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    history = []
    optimizer = opt_func(model.parameters(), lr)
    for epoch in range(epochs):
        # Training Phase 
        model.train()
        train_losses = []
        for batch in tqdm(train_loader):
            loss = model.training_step(batch)
            train_losses.append(loss)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        # Validation phase
        result = evaluate(model, val_loader)
        result['train_loss'] = torch.stack(train_losses).mean().item()
        model.epoch_end(epoch, result)
        history.append(result)
    return history

class Model(nn.Module):
    def __init__(self, in_size, layer_array = [512, 512, 256, 256], out_size = 28):
        super().__init__()
        self.network = nn.Sequential(
          nn.Linear(in_size, layer_array[0]),
          nn.ReLU(),
          # nn.Dropout(dropoutProb),
          nn.Linear(layer_array[0], layer_array[1]),
          nn.ReLU(),
          # nn.Dropout(dropoutProb),
          nn.Linear(layer_array[1], layer_array[2]),
          nn.ReLU(),
          # nn.Dropout(dropoutProb),
          nn.Linear(layer_array[2], layer_array[3]),
          nn.ReLU(),
          # nn.Dropout(dropoutProb),
          nn.Linear(layer_array[3], out_size)
        )
        
    def forward(self, xb):
        softmax = nn.LogSoftmax(dim=0)
        return softmax(self.network(xb))

    def training_step(self, batch):
        values, labels = batch
        values = values.to(device)
        labels = labels.to(device)
        out = self(values)                  # Generate predictions
        loss = nn.functional.nll_loss(out, labels) # Calculate loss
        return loss
    
    def validation_step(self, batch):
        values, labels = batch 
        values = values.to(device)
        labels = labels.to(device)
        out = self(values)                    # Generate predictions
        loss = nn.functional.nll_loss(out, labels)   # Calculate loss
        acc = accuracy(out, labels)           # Calculate accuracy
        return {'val_loss': loss, 'val_acc': acc}
        
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}".format(epoch, result['val_loss'], result['val_acc']))

    def predict(value, model):
        xb = to_device(value, device)
        yb = model(xb)  
        _, preds  = torch.max(yb, dim=0)
        return preds.item()

	

# Add Cuda availability
print('\n======Model 0.5.0=======\n')
print(f'torch vesrion:{torch.__version__}  cuda availability:{torch.cuda.is_available()} with {torch.cuda.device_count()} GPU devices')
device = get_default_device()
print(f'{device} set as the default device')

k = 7
inputFeatures = int((4**k) / 2) + 15
layer_array = [512, 512, 256, 256]
outputSize = 2
momentum = 0.4
dropoutProb = 0.2
batchSize = 200
num_epochs = 10
opt_func = torch.optim.Adam
lr = 0.001
num_workers = 10

print('Importing the dataset....')
#trainingDataset = HDF5Dataset('/home/anuvini/Documents/FYPML/Plasbin_datasets/training.h5', False, data_cache_size=400000,label_threshold=6)
trainingDataset = HDF5Dataset('/home/anuvini/Documents/FYPML/Plasbin_datasets/testing.h5', True, data_cache_size=1000,label_threshold=6)
datasetsize = len(trainingDataset)

train_size = int(0.8 * len(trainingDataset))
val_size = len(trainingDataset) - train_size

print('\nSplitting Training/Validation datasets....')
train_ds, val_ds = random_split(trainingDataset, [train_size, val_size])
print('==== Dataset processed ====')

train_dl = DataLoader(train_ds, batchSize, shuffle=True, num_workers=num_workers, pin_memory=True)
val_dl = DataLoader(val_ds, batchSize, num_workers=num_workers, shuffle =True, pin_memory=True)

model = Model(inputFeatures, layer_array, outputSize)
model = to_device(model, device)

model.double()
history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)

plot_accuracies(history)

plot_losses(history)

'''
test_dl = DataLoader(testingDataset, batchSize, num_workers=4, pin_memory=True)

cor = []
incor = []
for i in range(580):
    prediction = predict(testingDataset[i][0], model)
    label = testingDataset[i][1].item()
    # print(label)
    if(prediction == label):
        cor.append([label, prediction])
    else:
        incor.append([label, prediction])

correct_df = pd.DataFrame(cor, columns=['label', 'prediction'])
incorrect_df = pd.DataFrame(incor, columns=['label', 'prediction'])

print(f'correct:{len(correct_df['prediction'].value_counts())}')
print(f'incorrect:{len(incorrect_df['prediction'].value_counts())}')

incorrect_df.to_csv('incorrect.csv')
correct_df.to_csv('correct.csv
'''